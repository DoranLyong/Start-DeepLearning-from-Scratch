{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax-with-Loss 계층 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**그동안 다룬 내용**\n",
    ">1. 계산 그래프<sup>(computational graph)</sup>로 연산 표현하기 \n",
    ">2. 계산 그래프로 역전파 이해\n",
    ">3. scalar 신호 역전파  \n",
    "  * 곱셈 계층, 덧셈 계층 \n",
    "  * 활성화 함수 계층(ReLU, Sigmoid)\n",
    "  \n",
    ">4. 배열 신호 역전파 \n",
    "  * Affine 계층 \n",
    "  * batch용 Affine 계층 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 마지막으로 출력층에서 사용하는 softmax function에 관해 정리함 <br/>\n",
    "\n",
    "> Softmax 함수는 입력 값을 정규화하여 출력한다(page. 94, 101) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 손글씨 숫자 인식에서의 Softmax 계층 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<간소화 이미지>\n",
    "![간소화이미지](./images/fig_5-28.png)\n",
    "\n",
    "* 입력 이미지가 Affine 계층과 ReLU 계층을 통과하여 변환 \n",
    "* 마지막 Softmax 계층에 의해서 10개의 입력이 정규화됨 ($\\Sigma$출력 = 1)\n",
    "> * 이 그림에서 '0'의 점수는 5.3 \n",
    "> * 이것이 Softmax 계층을 지나 0.008(0.8%)로 변환  \n",
    "\n",
    "\n",
    "### 참고 \n",
    "<span style=\"color:blue\">분류할 클래스 개수</span> = 출력 노드 개수 \n",
    "\n",
    "따라서, 숫자 가짓수가 10개 &rarr; Softmax 계층의 입력도 10개 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 각 계층은 세부적으로 다음과 같은 모양임  \n",
    "![](./images/fig_normalized.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 알아 둘 것 (p.176)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 신경망에서 수행하는 작업은 __학습__<sup>learning</sup>과 __추론__<sup>inference</sup> 두 가지임 (p. 95) <br/>\n",
    "\n",
    "2. 추론할 때: 일반적으로 Softmax 계층 생략 \n",
    "> * Softmax 계층 앞의 Affine 계층의 출력을 인식 결과로 활용\n",
    "> * 이를 __점수__<sup>score</sup>라 함 \n",
    "> * 왜냐면, 신경망 추론에서 답을 하나만 내는 경우 <span style=\"color:blue\">가장 높은 점수만 알면 되니까</span>(= Softmax 계층이 불필요)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 신경망을 학습할 때는 Softmax 계층이 필요 \n",
    "> * 정규화된 데이터로 손실 함수<sup>loss function</sup>을 구하려고 (p. 102, 111)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax-with-Loss 계층 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "손실 함수를 포함하여 소프트맥스 계층 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Soft-with-Loss 계층의 계산 그래프 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/fig_a-5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Softmax__  <br/>\n",
    "$y_{k}=\\frac{exp(a_k)}{\\Sigma exp(a_k)}$  <br/>\n",
    "\n",
    "__CEE__ <br/>\n",
    "$E=-\\Sigma t_{k}\\log y_{k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "수식 도출 과정은 \"부록 A. Softmax-with-Loss 계층의 계싼 그래프 \" 참고 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 간소화한 Softmax-with-Loss 계층의 계산 그래프 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/fig_5-30.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward\n",
    "* 여기서는 3클래스 분류를 가정하고 이전 계층에서 3개의 입력(__점수<sup>score</sup>__)을 받음 \n",
    "> ($a_{1}$, $a_{2}$, $a_{3}$) <br/>\n",
    "\n",
    "* Softmax 계층은 입력 ($a_{1}$, $a_{2}$, $a_{3}$)을 정규화하여 ($y_{1}$, $y_{2}$, $y_{3}$)를 출력 <br/>\n",
    "\n",
    "\n",
    "* CEE 계층은 출력($y_{1}$, $y_{2}$, $y_{3}$)와 정답 레이블 ($t_{1}$, $t_{2}$, $t_{3}$)를 받음 \n",
    "> 이들 데이터로부터 손실 L을 출력함 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward\n",
    "* Softmax 계층의 역전파<sup>backpropagation</sup>는 ($y_{1}-t_{1}$, $y_{2}-t_{2}$, $y_{3}-t_{3}$)라는 '말끔한' 결과를 내놓음 \n",
    "> 즉, Softmax 계층의 출력과 정답 레이블의 차분임 \n",
    "   * 신경망의 역전파에서 이러한 차이(=오차)가 앞 계층에 전해지는 것 \n",
    "  \n",
    "###### 신경망 학습의 중요한 성질이니까 기억하자 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__그런데__ <br/>\n",
    "\n",
    "신경망 학습의 목적: \n",
    "> 신경망의 출력<sup>(Softmax의 출력)</sup>이 정답 레이블과 가까워지도록 가중치 매개변수의 값을 조정하는 것 \n",
    "\n",
    "그래서 신경망의 출력($y_{k}$)과 정답 레이블($t_{k}$)간의 오차를 효율적으로 앞 계층에 전달해야함 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 그렇다고 봤을 때, \n",
    "($y_{1}-t_{1}$, $y_{2}-t_{2}$, $y_{3}-t_{3}$)라는 결과는... \n",
    "> 신경망의 현재 출력과 정답 레이블의 오차(error)를 있는 그대로 드러냄 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import sys, os \n",
    "sys.path.append( os.pardir ) # to import modules from 'common' directory\n",
    "                            # (Book page. 97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.functions import cross_entropy_error, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss: \n",
    "    def __init__(self):\n",
    "        self.loss = None   # 손실 \n",
    "        self.y    = None   # softmax 출력 \n",
    "        self.t    = None   # 정답 레이블 (one-hot 벡터) \n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t    = t \n",
    "        self.y    = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t) \n",
    "        return self.loss \n",
    "    \n",
    "    def backward(self, dout = 1): \n",
    "        batch_size = self.t.shape[0]  \n",
    "        dx         = (self.y - self.t) / batch_size \n",
    "        \n",
    "        return dx \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \"소프트맥스 함수 구현 시 주의점\" (p.93) \n",
    "* \"(배치용) CEE 구현하기\" (p. 119)            참고 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "역전파 때는 전파하는 값을 배치의 수(batch_size)로 나눔 \n",
    "> 데이터 1개당 오차를 앞 계층으로 전파함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
